{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q langchain\n","!pip install -q torch\n","!pip install -q transformers\n","!pip install -q sentence-transformers\n","!pip install -q datasets\n","!pip install -q faiss-cpu\n","!pip install gpt4all\n","!pip install chromadb\n","!pip install langchainhub\n","!pip install openai\n","!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"skxGT5LUTFUG","executionInfo":{"status":"ok","timestamp":1714761650540,"user_tz":300,"elapsed":171608,"user":{"displayName":"Naga Hema Chand Chinta","userId":"14865467068698116185"}},"outputId":"3c4d26d3-c074-4f51-c7b4-2477dfb1608b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gpt4all\n","  Downloading gpt4all-2.6.0-py3-none-manylinux1_x86_64.whl (3.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.2)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2024.2.2)\n","Installing collected packages: gpt4all\n","Successfully installed gpt4all-2.6.0\n","Collecting chromadb\n","  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n","Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.1)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n","Collecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.11.0)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.63.0)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.3)\n","Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n","  Downloading fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n","Collecting httpx>=0.23.0 (from fastapi>=0.95.2->chromadb)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.3)\n","Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n","  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n","  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n","Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n","Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n","Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n","Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.18.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.7)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.0)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typer>=0.9.0 (from chromadb)\n","  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (3.7.1)\n","Collecting httpcore==1.* (from httpx>=0.23.0->fastapi>=0.95.2->chromadb)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.2.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n","Building wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=c7580d1c1e83ec2433294ab56a9ff4aa1a6982fc506d93ba228b516478f53acd\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, ujson, shellingham, python-multipart, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, dnspython, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpcore, email_validator, coloredlogs, typer, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, fastapi-cli, fastapi, chromadb\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 7.1.0\n","    Uninstalling importlib_metadata-7.1.0:\n","      Successfully uninstalled importlib_metadata-7.1.0\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.4\n","    Uninstalling typer-0.9.4:\n","      Successfully uninstalled typer-0.9.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 ujson-5.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n","Collecting langchainhub\n","  Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n","Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n","Installing collected packages: types-requests, langchainhub\n","Successfully installed langchainhub-0.1.15 types-requests-2.31.0.20240406\n","Collecting openai\n","  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n","Installing collected packages: openai\n","Successfully installed openai-1.25.1\n","Collecting tiktoken\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.6.0\n"]}]},{"cell_type":"code","source":["# Used to securely store your API key\n","from google.colab import userdata"],"metadata":{"id":"rhLNyoQi-Ljt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["HUGGINGFACE_API_KEY=userdata.get('HUGGINGFACE_API_KEY')\n","HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')"],"metadata":{"id":"J3GHnx7X-U_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ['HUGGINGFACE_API_KEY'] = HUGGINGFACE_API_KEY\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN"],"metadata":{"id":"dZP44FLt7uUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTkcsfeu-mn7","executionInfo":{"status":"ok","timestamp":1714761757121,"user_tz":300,"elapsed":105783,"user":{"displayName":"Naga Hema Chand Chinta","userId":"14865467068698116185"}},"outputId":"3edf15b0-04e4-4ebd-d2c7-5398418d3a0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import ast\n","from langchain.document_loaders import TextLoader\n","from langchain_community.document_loaders.csv_loader import CSVLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","from langchain.embeddings import GPT4AllEmbeddings\n","from langchain.llms import Ollama\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain.chains import StuffDocumentsChain, RetrievalQA\n","from langchain import hub\n","from langchain.chat_models import ChatOpenAI\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","from transformers import AutoTokenizer, pipeline\n","from langchain import HuggingFacePipeline\n","from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n","from langchain.memory import VectorStoreRetrieverMemory\n","from langchain.storage import InMemoryByteStore\n","from langchain.retrievers.multi_vector import MultiVectorRetriever\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain.chains import create_retrieval_chain\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.documents import Document\n","\n","import re\n","import csv\n","import uuid\n","\n","class Document:\n","  def __init__(self, content, metadata=None):\n","        self.page_content = content\n","        self.metadata = metadata if metadata is not None else {}\n","\n","def main(file_path):\n","\n","\n","  #-------------------DATA LOADING----------------------------------------------\n","  #Air_dialogue data description (Ref: https://huggingface.co/datasets/google/air_dialogue?row=4):\n","  ##The Air_dialogue dataset basically consists of\n","    ## 'Key' : Description\n","    # 'search_action' :\tsearch action performed by customer\n","    # 'action' :\tAction taken by the agent\n","    # 'intent'\t: Intents from the conversation\n","    # 'timestamps'\t: Timestamp for each of the dialogues\n","    # 'dialogue'\t: Dialogue recorded between agent & customer\n","    # 'expected_action'\t: Expected action from agent (human-annotated)\n","    # 'correct_sample' :\twhether action performed by agent was same as expected_action\n","\n","\n","  Loader = CSVLoader(file_path)\n","  data = Loader.load()\n","  docs = []\n","  text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n","  docs = text_splitter.split_documents(data)\n","\n","\n","  # #------------------Loading only SUMMARY as a Data-----------------------------\n","  #Air_dialogue processed data description:\n","  ##In the above metadata, we filtered out the unwanted data and only focused on action and dialogue (this has all the utterances between the agent and the customer).\n","  # The log key has the dialogue and the prompt key gets the user input prompt and the external knowledge is the summary which is summarisation of the dialogue using an LLM.\n","\n","  sum_file_path = '/content/drive/MyDrive/Hemachand/Datasets/Airdialogue/From_code_sub_air_dialogue_validation.csv'\n","  sum_Loader = CSVLoader(sum_file_path)\n","  sum_data = sum_Loader.load()\n","  sum_docs = []\n","  sum_docs = text_splitter.split_documents(sum_data)\n","\n","\n","  #------------------CREATING EMBEDDINGS----------------------------------------\n","  # Define the path to the pre-trained model you want to use\n","  modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n","\n","  # Create a dictionary with model configuration options, specifying to use the CPU for computations\n","  model_kwargs = {'device':'cpu'}\n","\n","  # Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n","  encode_kwargs = {'normalize_embeddings': False}\n","\n","  # Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n","  embeddings = HuggingFaceEmbeddings(\n","      model_name=modelPath,     # Provide the pre-trained model's path\n","      model_kwargs=model_kwargs, # Pass the model configuration options\n","      encode_kwargs=encode_kwargs # Pass the encoding options\n","  )\n","\n","  #----------------------VECTORSTORE--------------------------------------------\n","  # The vectorstore to use to index the child chunks\n","  vectorstore = Chroma(\n","      collection_name=\"full_documents\", embedding_function=embeddings\n","  )\n","\n","  #----------------------RETRIEVER----------------------------------------------\n","  # The storage layer for the parent documents\n","  store = InMemoryByteStore()\n","  id_key = \"doc_id\"\n","  # The retriever (empty to start)\n","  retriever = MultiVectorRetriever(\n","      vectorstore=vectorstore,\n","      byte_store=store,\n","      id_key=id_key,\n","  )\n","\n","  doc_ids = [str(uuid.uuid4()) for _ in docs]\n","\n","  #----------------------CHILD TEXT SPLITTER FOR DOC-1 (TURNS)------------------\n","  # The splitter to use to create smaller chunks\n","  child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n","\n","\n","  sub_docs = [] #This will be the child document\n","  for i, doc in enumerate(docs):\n","      _id = doc_ids[i]\n","      _sub_docs = child_text_splitter.split_documents([doc])\n","      for _doc in _sub_docs:\n","          _doc.metadata[id_key] = _id\n","      sub_docs.extend(_sub_docs)\n","\n","\n","  #----------------------CHILD TEXT SPLITTER FOR DOC-2 (SUMMARY)----------------\n","  sum_doc_ids = [str(uuid.uuid4()) for _ in sum_docs]\n","  sub_sum_docs = [] #This will be the child document for summary\n","  for i, doc in enumerate(sum_docs):\n","      # print(\"i\", i)\n","      # print(\"doc\", doc)\n","      _id = sum_doc_ids[i]\n","      _sub_sum_docs = child_text_splitter.split_documents([doc])\n","      for _doc in _sub_sum_docs:\n","          _doc.metadata[id_key] = _id\n","      sub_sum_docs.extend(_sub_sum_docs)\n","\n","\n","  #-------------ADD Documents To RETRIEVER--------------------------------------\n","# retriever.vectorstore.add_documents(sub_docs)\n","  retriever.vectorstore.add_documents(sub_sum_docs)\n","  retriever.docstore.mset(list(zip(doc_ids, docs)))\n","\n","\n","  #-------------LLM Initialization----------------------------------------------\n","  llm=HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",model_kwargs={\"temperature\":0.5,\n","                                                  \"max_length\":64})\n","\n","  #-------------QA Prompt-------------------------------------------------------\n","  # QA_CHAIN_template = \"\"\"\n","  # If you don't find the relavant information from the document retreived, say \"I don't have relevant information\" and give precise and accurate information, else when you find relavant information, only respond back with a couple of words or sentences answering the query and don't give the entire conversational response, let the user respond for the conversation. The information to be responded is based on the following query.\n","  # {query}\n","  # \"\"\"\n","  # QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"query\"], template = QA_CHAIN_template)\n","\n","  # retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n","\n","  retrieval_qa_chat_prompt = ChatPromptTemplate.from_messages(\n","      [(\"system\",\"You are travel agent who gives information and conversational AI bot\"),\n","       (\"human\",\"Give complete response for the following \\n\\n{input} based on the following \\n\\n{context} starting your response with Answer:\")]\n","  )\n","\n","  #-------------RETREIVER CHAIN-------------------------------------------------\n","  combine_docs_chain = create_stuff_documents_chain(\n","    llm, retrieval_qa_chat_prompt\n","  )\n","  retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n","\n","\n","\n","  #-------------CHATBOT---------------------------------------------------------\n","  # Start a loop for the chatbot interaction\n","  while True:\n","      # Get user input\n","      user_query = input(\"Ask me something (type 'exit' to quit): \")\n","      # cntxt = \"Retreive the relevant information from the retreiver\"\n","\n","      # Check if the user wants to exit\n","      if user_query.lower() in ['exit', 'quit']:\n","          print(\"Goodbye!\")\n","          break\n","\n","      # Get the response from the chatbot\n","      response = retrieval_chain.invoke({\"input\": user_query}, {\"context\": sub_docs})\n","      # response = qa_chain(user_query)\n","\n","      # Print the response\n","      # print(response['query'])\n","      # print(response['text'].strip())\n","      # print(response)\n","\n","      # # Find the index where \"AI:\" starts\n","      # start_index = response['answer'].find('AI:')\n","      # print(\"Chatbot:\" + response['answer'][start_index + 3:])\n","\n","      # Locate the start of the \"Response:\" text\n","      response_start_index = response['answer'].find(\"Response:\")\n","      print(\"Chatbot: \" + response['answer'][response_start_index + 10:])\n","\n","\n","if __name__ == \"__main__\":\n","    file_path = '/content/drive/MyDrive/Hemachand/Datasets/Airdialogue/sub_air_dialogue_validation.csv'\n","    main(file_path)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XS9qPYAxUNIi","executionInfo":{"status":"ok","timestamp":1714776327056,"user_tz":300,"elapsed":5414520,"user":{"displayName":"Naga Hema Chand Chinta","userId":"14865467068698116185"}},"outputId":"8598a44e-177f-42b2-ea6a-f7a75a635437"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Ask me something (type 'exit' to quit): Show flights\n","Chatbot: ou are travel agent who gives information and conversational AI bot\n","Human: Give complete response for the following \n","\n","Show flights based on the following \n","\n"," starting your response with Answer:\n","\n","Answer: I'd be happy to help you find flights based on your preferences. To get started, could you please provide me with the following details:\n","\n","1. Departure city and airport code (e.g., New York - JFK)\n","2. Arrival city and airport code (e.g., Los Angeles - LAX)\n","3. Departure date\n","4. Return date (if applicable)\n","5. Number of passengers\n","Ask me something (type 'exit' to quit): book a flight from minneapolis to mars\n","Chatbot: ou are travel agent who gives information and conversational AI bot\n","Human: Give complete response for the following \n","\n","book a flight from minneapolis to mars based on the following \n","\n"," starting your response with Answer:\n","\n","Answer: I'm sorry for any confusion, but currently, it's not possible to book a flight from Minneapolis to Mars. Commercial space travel to Mars is still in its developmental stages and isn't open to the public. SpaceX, the most advanced commercial space travel company, plans to send its first crewed mission to Mars in the 2030s. Please check back with us for future updates on space travel options. In the meantime,\n","Ask me something (type 'exit' to quit): I want to book a flight on Sept 20 2022\n","Chatbot: ou are travel agent who gives information and conversational AI bot\n","Human: Give complete response for the following \n","\n","I want to book a flight on Sept 20 2022 based on the following \n","\n"," starting your response with Answer:\n","\n","Answer: I'd be happy to help you find a flight for your desired travel date, which is September 20, 2022. To provide you with the best options, could you please specify the following details:\n","\n","1. Your departure city.\n","2. Your destination city.\n","3. Your preferred cabin class.\n","4. Any airline preferences.\n","5. Your budget for the flight.\n","\n","Once I have this information, I\n","Ask me something (type 'exit' to quit): I want to book a flight to LGA from DEN and two more from LGA to DTW via DEN but from DEN to DTW only one instead of 2 tickets\n","Chatbot: ou are travel agent who gives information and conversational AI bot\n","Human: Give complete response for the following \n","\n","I want to book a flight to LGA from DEN and two more from LGA to DTW via DEN but from DEN to DTW only one instead of 2 tickets based on the following \n","\n"," starting your response with Answer:\n","\n","Answer: I understand that you'd like to book a round trip from Denver International Airport (DEN) to LaGuardia Airport (LGA), and then another one-way flight from LGA to Detroit Metropolitan Wayne County Airport (DTW) via Denver International Airport (DEN). However, instead of booking two separate tickets from DEN to DTW, you'd prefer one ticket. I'd be happy to help you with that. Here's a\n","Ask me something (type 'exit' to quit): I want to book a flight to LGA from DEN and two more from LGA to DTW via DEN but from DEN to DTW only one instead of 2 tickets. Give the flights options directly\n","Chatbot: ou are travel agent who gives information and conversational AI bot\n","Human: Give complete response for the following \n","\n","I want to book a flight to LGA from DEN and two more from LGA to DTW via DEN but from DEN to DTW only one instead of 2 tickets. Give the flights options directly based on the following \n","\n"," starting your response with Answer:\n","\n","Answer: I'd be happy to help you with your travel plans! Based on your request, I've found the following flight options for you:\n","\n","1. From DEN to LGA:\n","   - Departure Date: [Date]\n","   - Departure Time: [Time]\n","   - Arrival Date: [Date]\n","   - Arrival Time: [Time]\n","   - Airline: [Airline]\n","\n","Ask me something (type 'exit' to quit): quit\n","Goodbye!\n"]}]}]}